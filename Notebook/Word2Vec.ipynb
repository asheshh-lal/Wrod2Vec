{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14bba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7567d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/simpsons_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539d210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158309</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158311</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158312</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158313</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "158309              Miss Hoover   \n",
       "158310              Miss Hoover   \n",
       "158311              Miss Hoover   \n",
       "158312             Ralph Wiggum   \n",
       "158313                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "158309                                          I'm back.  \n",
       "158310  You see, class, my Lyme disease turned out to ...  \n",
       "158311                                 Psy-cho-so-ma-tic.  \n",
       "158312                     Does that mean you were crazy?  \n",
       "158313                  No, that means she was faking it.  \n",
       "\n",
       "[158314 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecaa21",
   "metadata": {},
   "source": [
    "### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379107e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b788211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1808bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131848</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131849</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131850</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131851</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131852</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131853 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "131848              Miss Hoover   \n",
       "131849              Miss Hoover   \n",
       "131850              Miss Hoover   \n",
       "131851             Ralph Wiggum   \n",
       "131852                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "131848                                          I'm back.  \n",
       "131849  You see, class, my Lyme disease turned out to ...  \n",
       "131850                                 Psy-cho-so-ma-tic.  \n",
       "131851                     Does that mean you were crazy?  \n",
       "131852                  No, that means she was faking it.  \n",
       "\n",
       "[131853 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03ada0",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c14339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the smaller English model, disabling Named Entity Recognition and parsing\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # If the cleaned text has more than two words, return it\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f05e5",
   "metadata": {},
   "source": [
    "### Remove non alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2da0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba14196",
   "metadata": {},
   "source": [
    "### Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:\n",
    "This line of code processes text data using an NLP (Natural Language Processing) pipeline, with the following key parts:\n",
    "\n",
    "1. nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1):\n",
    "nlp.pipe(): This is a method from an NLP library like SpaCy that allows you to process a large set of documents in a memory-efficient and fast way by processing them in batches.\n",
    "\n",
    "brief_cleaning: This is the input data, which is likely a list of text documents that need to be processed. Each document in this list is passed through the nlp pipeline, which might involve tokenization, part-of-speech tagging, named entity recognition, and other NLP tasks depending on what the pipeline is set up to do.\n",
    "\n",
    "batch_size=5000: The documents in brief_cleaning are processed in batches of 5000 at a time, which can improve efficiency and reduce memory usage when processing large datasets.\n",
    "\n",
    "n_process=-1: This argument controls parallel processing. Setting n_process=-1 means that the code will use all available CPU cores to process the documents in parallel, speeding up the task.\n",
    "\n",
    "2. cleaning(doc):\n",
    "For each document (doc) returned by nlp.pipe(), a custom function named cleaning() is applied. This function likely performs some form of text preprocessing or cleaning, such as removing stop words, punctuation, or transforming the text in some other way (e.g., lowercasing or lemmatization).\n",
    "3. txt = [ ... ]:\n",
    "This list comprehension runs the cleaning() function on each document in the pipeline. The results (cleaned documents) are stored in the list txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec12832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.19 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a04353",
   "metadata": {},
   "source": [
    "### Put the results in a DataFrame to remove missing values and duplicates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb00f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85955, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487e11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fb979e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually little disease magazine news show nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know sure like talk touch lesson plan teach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>life worth live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poll open end recess case decide thought final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victory party slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131829</th>\n",
       "      <td>oh mom wonderful find favorite dish help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131835</th>\n",
       "      <td>dye shoe pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131846</th>\n",
       "      <td>mr bergstrom request pleasure company mr bergs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131849</th>\n",
       "      <td>class lyme disease turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131850</th>\n",
       "      <td>psy cho ma tic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85955 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    clean\n",
       "0       actually little disease magazine news show nat...\n",
       "2             know sure like talk touch lesson plan teach\n",
       "3                                         life worth live\n",
       "4       poll open end recess case decide thought final...\n",
       "7                                     victory party slide\n",
       "...                                                   ...\n",
       "131829           oh mom wonderful find favorite dish help\n",
       "131835                                      dye shoe pink\n",
       "131846  mr bergstrom request pleasure company mr bergs...\n",
       "131849                            class lyme disease turn\n",
       "131850                                     psy cho ma tic\n",
       "\n",
       "[85955 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ebe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d55a5911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually little disease magazine news show nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know sure like talk touch lesson plan teach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>life worth live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poll open end recess case decide thought final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victory party slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131829</th>\n",
       "      <td>oh mom wonderful find favorite dish help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131835</th>\n",
       "      <td>dye shoe pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131846</th>\n",
       "      <td>mr bergstrom request pleasure company mr bergs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131849</th>\n",
       "      <td>class lyme disease turn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131850</th>\n",
       "      <td>psy cho ma tic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85955 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    clean\n",
       "0       actually little disease magazine news show nat...\n",
       "2             know sure like talk touch lesson plan teach\n",
       "3                                         life worth live\n",
       "4       poll open end recess case decide thought final...\n",
       "7                                     victory party slide\n",
       "...                                                   ...\n",
       "131829           oh mom wonderful find favorite dish help\n",
       "131835                                      dye shoe pink\n",
       "131846  mr bergstrom request pleasure company mr bergs...\n",
       "131849                            class lyme disease turn\n",
       "131850                                     psy cho ma tic\n",
       "\n",
       "[85955 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daea886",
   "metadata": {},
   "source": [
    "### This line of code is using the **`Phrases`** class from the **Gensim** library, which is commonly used to detect multi-word expressions (also known as \"collocations\" or \"phrases\") in a text corpus. Here's a breakdown of what this specific line does:\n",
    "\n",
    "### 1. **`Phrases(sent)`**:\n",
    "   - **`Phrases`** is a Gensim function that identifies common phrases (e.g., \"New York\", \"machine learning\") in a list of tokenized sentences. \n",
    "   - **`sent`** is the input to the `Phrases` class. It is a list of tokenized sentences, where each sentence is a list of words (in this case, it was created from the previous code snippet: `sent = [row.split() for row in df_clean['clean']]`).\n",
    "   \n",
    "     For example:\n",
    "     ```python\n",
    "     sent = [['this', 'is', 'a', 'test'], ['another', 'example', 'here']]\n",
    "     ```\n",
    "\n",
    "   Gensim will look through these sentences to identify common multi-word expressions (phrases) like \"New York\" or \"machine learning\" that often occur together.\n",
    "\n",
    "### 2. **`min_count=30`**:\n",
    "   - The `min_count` parameter sets a threshold for phrase detection. In this case, only phrases that appear **at least 30 times** in the corpus (the list of sentences `sent`) will be considered as valid phrases.\n",
    "   \n",
    "     For example, if \"machine learning\" appears together 30 or more times in the dataset, it will be identified as a phrase. If it appears fewer than 30 times, it won't be considered a phrase.\n",
    "\n",
    "### 3. **`progress_per=10000`**:\n",
    "   - This parameter controls how often the progress of the phrase detection process is reported (useful for large datasets). It means that Gensim will output progress after every 10,000 sentences it processes, providing updates on how far along it is in detecting phrases.\n",
    "\n",
    "### 4. **`phrases = ...`**:\n",
    "   - The result of applying the `Phrases` class is stored in the variable `phrases`. This object contains information about the detected phrases and can be used to transform the tokenized sentences so that multi-word expressions are combined into single tokens.\n",
    "\n",
    "### Example of Detected Phrases:\n",
    "After running this code, if the phrases \"machine learning\" or \"New York\" are detected in the dataset, Gensim might combine those into single tokens, such as `machine_learning` and `New_York`. \n",
    "\n",
    "For example, the input sentence:\n",
    "```python\n",
    "['I', 'am', 'learning', 'machine', 'learning']\n",
    "```\n",
    "could be transformed into:\n",
    "```python\n",
    "['I', 'am', 'learning', 'machine_learning']\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "The line of code `phrases = Phrases(sent, min_count=30, progress_per=10000)` creates a model to detect multi-word expressions that occur frequently in the tokenized text data. Only phrases that appear at least 30 times will be considered, and the progress will be reported after every 10,000 sentences processed. The result, stored in `phrases`, is an object that can later be used to transform sentences by combining detected phrases into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5768c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:57:07: collecting all words and their counts\n",
      "INFO - 09:57:07: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 09:57:07: collected 17 token types (unigram + bigrams) from a corpus of 11 words and 4 sentences\n",
      "INFO - 09:57:07: merged Phrases<17 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 09:57:07: Phrases lifecycle event {'msg': 'built Phrases<17 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.01s', 'datetime': '2024-10-19T09:57:07.505655', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd680e50",
   "metadata": {},
   "source": [
    "### The line of code sentences = bigram[sent] applies a bigram model to a list of tokenized sentences and produces new sentences where common bigrams (frequent two-word phrases) are combined into single tokens.\n",
    "\n",
    "Here's what it does in detail:\n",
    "1. bigram:\n",
    "bigram is a model that identifies common two-word phrases (bigrams) in text.\n",
    "This model is usually created from a Gensim Phrases object. For example, after detecting frequent bigrams using Phrases(sent, min_count=30), you can convert it to a more efficient form with:\n",
    "python\n",
    "Copy code\n",
    "bigram = Phraser(phrases)\n",
    "2. bigram[sent]:\n",
    "sent is a list of tokenized sentences, where each sentence is a list of words (tokens). For example:\n",
    "python\n",
    "Copy code\n",
    "sent = [['this', 'is', 'a', 'test'], ['machine', 'learning', 'is', 'fun']]\n",
    "When you apply bigram[sent], the bigram model is applied to every sentence in sent. It looks for common two-word phrases (bigrams) like \"machine learning\" and combines them into a single token like \"machine_learning\".\n",
    "As a result, the output sentences will contain the modified sentences where the detected bigrams are replaced by single tokens.\n",
    "Example:\n",
    "Suppose you have a sentence like:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "sent = [['machine', 'learning', 'is', 'fun'], ['this', 'is', 'a', 'test']]\n",
    "If the bigram model detects that \"machine learning\" is a common phrase, applying bigram[sent] will transform the sentence as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "sentences = [['machine_learning', 'is', 'fun'], ['this', 'is', 'a', 'test']]\n",
    "3. sentences:\n",
    "The variable sentences now holds the list of sentences where two-word phrases (bigrams) have been combined into single tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7882c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:57:09: exporting phrases from Phrases<17 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 09:57:09: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<0 phrases, min_count=30, threshold=10.0> from Phrases<17 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.00s', 'datetime': '2024-10-19T09:57:09.940939', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4d623",
   "metadata": {},
   "source": [
    "### Most Frequent Words:¶\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams.\n",
    "\n",
    "This code snippet calculates the frequency of each word (or token) in the list of tokenized sentences and then determines how many unique words (tokens) exist in the dataset. Here’s a breakdown of how it works:\n",
    "\n",
    "1. word_freq = defaultdict(int):\n",
    "This initializes a defaultdict (from Python’s collections module) to store the frequency of each word.\n",
    "A defaultdict works like a regular Python dictionary, but it provides a default value for keys that don’t exist. In this case, int initializes any missing key with a default value of 0.\n",
    "2. for sent in sentences::\n",
    "This loop iterates over each sentence (which is a list of tokens) in sentences.\n",
    "sentences is expected to be a list of tokenized sentences (after applying the bigram model in the previous steps).\n",
    "3. for i in sent::\n",
    "This inner loop iterates over each word/token i in the sentence sent.\n",
    "4. word_freq[i] += 1:\n",
    "For each token i, the code increments its frequency count by 1 in the word_freq dictionary.\n",
    "If the token i has not been seen before, the defaultdict initializes it with a value of 0, and then the count is incremented to 1. If the token has already been encountered, its count is simply incremented by 1.\n",
    "5. len(word_freq):\n",
    "This returns the total number of unique words (tokens) in the dataset by calculating the length of the word_freq dictionary, which holds each unique word as a key.\n",
    "Example:\n",
    "If sentences contains the following tokenized sentences:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "sentences = [['machine_learning', 'is', 'fun'], ['machine_learning', 'is', 'cool']]\n",
    "The word_freq dictionary would look like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "word_freq = {\n",
    "    'machine_learning': 2,\n",
    "    'is': 2,\n",
    "    'fun': 1,\n",
    "    'cool': 1\n",
    "}\n",
    "Calling len(word_freq) would return 4 because there are 4 unique words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1392a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29694"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9197a034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'like', 'know', 'get', 'hey', 'think', 'come', 'right', 'look', 'want']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4312944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'actually': 422,\n",
       "             'little': 2098,\n",
       "             'disease': 45,\n",
       "             'magazine': 122,\n",
       "             'news': 249,\n",
       "             'show': 214,\n",
       "             'natural': 77,\n",
       "             'think': 3592,\n",
       "             'know': 4818,\n",
       "             'sure': 1198,\n",
       "             'like': 5601,\n",
       "             'talk': 931,\n",
       "             'touch': 191,\n",
       "             'lesson': 162,\n",
       "             'plan': 302,\n",
       "             'teach': 324,\n",
       "             'life': 1224,\n",
       "             'worth': 141,\n",
       "             'live': 760,\n",
       "             'poll': 20,\n",
       "             'open': 418,\n",
       "             'end': 461,\n",
       "             'recess': 10,\n",
       "             'case': 215,\n",
       "             'decide': 134,\n",
       "             'thought': 119,\n",
       "             'final': 105,\n",
       "             'statement': 21,\n",
       "             'martin': 121,\n",
       "             'victory': 30,\n",
       "             'party': 421,\n",
       "             'slide': 49,\n",
       "             'mr': 797,\n",
       "             'bergstrom': 17,\n",
       "             'hey': 3620,\n",
       "             'move': 165,\n",
       "             'morning': 238,\n",
       "             'new': 1325,\n",
       "             'job': 588,\n",
       "             'take': 901,\n",
       "             'copernicus': 5,\n",
       "             'costume': 62,\n",
       "             'train': 131,\n",
       "             'capital_city': 40,\n",
       "             'traditional': 19,\n",
       "             'environmentally': 3,\n",
       "             'sound': 297,\n",
       "             'yes': 1128,\n",
       "             'backbone': 7,\n",
       "             'country': 243,\n",
       "             'leland': 1,\n",
       "             'stanford': 4,\n",
       "             'drive': 434,\n",
       "             'golden': 53,\n",
       "             'spike': 14,\n",
       "             'promontory': 1,\n",
       "             'point': 378,\n",
       "             'thank': 1328,\n",
       "             'vote': 164,\n",
       "             'man': 2501,\n",
       "             'voting': 9,\n",
       "             'geek': 21,\n",
       "             'got': 20,\n",
       "             'right': 3412,\n",
       "             'girl': 677,\n",
       "             'sweat': 34,\n",
       "             'long': 807,\n",
       "             'couple': 202,\n",
       "             'people': 1527,\n",
       "             'milhouse': 502,\n",
       "             'recount': 3,\n",
       "             'want': 3178,\n",
       "             'way': 1678,\n",
       "             'mister': 110,\n",
       "             'president': 261,\n",
       "             'board': 84,\n",
       "             'track': 75,\n",
       "             'afternoon': 87,\n",
       "             'delight': 17,\n",
       "             'come': 3583,\n",
       "             'shelbyville': 85,\n",
       "             'parkville': 1,\n",
       "             'oh': 6453,\n",
       "             'mean': 1256,\n",
       "             'go': 2707,\n",
       "             'leave': 1055,\n",
       "             'ah': 841,\n",
       "             'sorry': 1228,\n",
       "             'lisa': 1724,\n",
       "             'substitute': 24,\n",
       "             'teacher': 218,\n",
       "             'fraud': 30,\n",
       "             'today': 739,\n",
       "             'wear': 393,\n",
       "             'gym': 53,\n",
       "             'short': 163,\n",
       "             'tomorrow': 358,\n",
       "             'speak': 240,\n",
       "             'french': 88,\n",
       "             'pretend': 93,\n",
       "             'run': 581,\n",
       "             'band': 151,\n",
       "             'saw': 12,\n",
       "             'god': 623,\n",
       "             'true': 333,\n",
       "             'lie': 378,\n",
       "             'good': 2918,\n",
       "             'need': 1819,\n",
       "             'project': 91,\n",
       "             'problem': 447,\n",
       "             'middle': 94,\n",
       "             'class': 324,\n",
       "             'anybody': 132,\n",
       "             'care': 544,\n",
       "             'abandon': 47,\n",
       "             'understand': 295,\n",
       "             'miss': 598,\n",
       "             'feel': 1042,\n",
       "             'rely': 8,\n",
       "             'guess': 720,\n",
       "             'mind': 393,\n",
       "             'alongside': 4,\n",
       "             'speed': 65,\n",
       "             'goodbye': 152,\n",
       "             'honey': 509,\n",
       "             'okay': 1876,\n",
       "             'read': 472,\n",
       "             'note': 100,\n",
       "             'throw': 364,\n",
       "             'big': 1146,\n",
       "             'bash': 20,\n",
       "             'champagne': 33,\n",
       "             'musician': 24,\n",
       "             'holy': 88,\n",
       "             'bart': 2685,\n",
       "             'bad': 1045,\n",
       "             'thing': 2068,\n",
       "             'happen': 675,\n",
       "             'alright': 52,\n",
       "             'allright': 2,\n",
       "             'spill': 37,\n",
       "             'milk': 125,\n",
       "             'mopey': 1,\n",
       "             'tell': 1862,\n",
       "             'father': 658,\n",
       "             'glad': 216,\n",
       "             'cry': 160,\n",
       "             'hate': 446,\n",
       "             'base': 95,\n",
       "             'emotion': 17,\n",
       "             'sir': 897,\n",
       "             'baboon': 16,\n",
       "             'realize': 181,\n",
       "             'say': 1598,\n",
       "             'whoa': 299,\n",
       "             'somebody': 199,\n",
       "             'bind': 30,\n",
       "             'day': 1761,\n",
       "             'believe': 718,\n",
       "             'hear': 995,\n",
       "             'marge': 2332,\n",
       "             'call': 583,\n",
       "             'stupid': 593,\n",
       "             'ugly': 111,\n",
       "             'smelly': 11,\n",
       "             'ape': 53,\n",
       "             'homer': 2932,\n",
       "             'allow': 120,\n",
       "             'hurt': 310,\n",
       "             'feeling': 193,\n",
       "             'little_girl': 144,\n",
       "             'upstairs': 32,\n",
       "             'confidence': 25,\n",
       "             'shake': 108,\n",
       "             'happy': 580,\n",
       "             'faith': 73,\n",
       "             'daddy': 328,\n",
       "             'hold': 459,\n",
       "             'look': 3377,\n",
       "             'forgive': 119,\n",
       "             'maybe': 1039,\n",
       "             'help': 1053,\n",
       "             'lose': 625,\n",
       "             'special': 338,\n",
       "             'lucky': 153,\n",
       "             'roof': 60,\n",
       "             'child': 789,\n",
       "             'time': 2558,\n",
       "             'bed': 274,\n",
       "             'lot': 819,\n",
       "             'probably': 212,\n",
       "             'place': 810,\n",
       "             'food': 378,\n",
       "             'real': 665,\n",
       "             'guy': 1581,\n",
       "             'serve': 114,\n",
       "             'drink': 418,\n",
       "             'explain': 113,\n",
       "             'fix': 147,\n",
       "             'doll': 87,\n",
       "             'house': 821,\n",
       "             'monkey': 214,\n",
       "             'work': 1332,\n",
       "             'nail': 58,\n",
       "             'tail': 44,\n",
       "             'dad': 1913,\n",
       "             'matter': 240,\n",
       "             'son': 936,\n",
       "             'lewis': 23,\n",
       "             'get': 4202,\n",
       "             'money': 943,\n",
       "             'neat': 21,\n",
       "             'ball': 280,\n",
       "             'world': 789,\n",
       "             'series': 69,\n",
       "             'huh': 556,\n",
       "             'let': 2900,\n",
       "             'baby': 888,\n",
       "             'bottle': 128,\n",
       "             'motto': 8,\n",
       "             'moly': 10,\n",
       "             'parent': 259,\n",
       "             'sleep': 376,\n",
       "             'maggie': 474,\n",
       "             'roll': 215,\n",
       "             'mmm': 143,\n",
       "             'hor': 8,\n",
       "             'doover': 1,\n",
       "             'promise': 264,\n",
       "             'eat': 946,\n",
       "             'go_to': 2524,\n",
       "             'pay': 610,\n",
       "             'friend': 841,\n",
       "             'invite': 101,\n",
       "             'home': 1025,\n",
       "             'mom': 1098,\n",
       "             'witty': 13,\n",
       "             'banter': 2,\n",
       "             'sophisticated': 18,\n",
       "             'adult': 120,\n",
       "             'yeah': 2305,\n",
       "             'fun': 566,\n",
       "             'old': 1085,\n",
       "             'well': 1098,\n",
       "             'hmmm': 177,\n",
       "             'mmmm': 58,\n",
       "             'gag': 31,\n",
       "             'ice': 106,\n",
       "             'cub': 7,\n",
       "             'record': 158,\n",
       "             'bartender': 36,\n",
       "             'ph': 6,\n",
       "             'd': 259,\n",
       "             'mixology': 1,\n",
       "             'try': 1014,\n",
       "             'flander': 381,\n",
       "             'planter': 1,\n",
       "             'punch': 114,\n",
       "             'alcohol': 64,\n",
       "             'au': 14,\n",
       "             'contraire': 3,\n",
       "             'simpson': 980,\n",
       "             'shot': 117,\n",
       "             'rum': 13,\n",
       "             'jigger': 2,\n",
       "             'bourbon': 11,\n",
       "             'dab': 6,\n",
       "             'roo': 3,\n",
       "             'creme': 6,\n",
       "             'de': 177,\n",
       "             'cassis': 1,\n",
       "             'flavor': 44,\n",
       "             'warm': 92,\n",
       "             'sense': 122,\n",
       "             'ssslurre': 1,\n",
       "             'shpeech': 1,\n",
       "             'easy': 335,\n",
       "             'al': 48,\n",
       "             'ky': 2,\n",
       "             'hol': 2,\n",
       "             'remember': 691,\n",
       "             'year': 984,\n",
       "             'winfield': 6,\n",
       "             'laundry': 35,\n",
       "             'hamper': 7,\n",
       "             'hi': 379,\n",
       "             'sister': 281,\n",
       "             'law': 206,\n",
       "             'beau': 4,\n",
       "             'tiful': 2,\n",
       "             'ow': 196,\n",
       "             'jeez': 32,\n",
       "             'kind': 553,\n",
       "             'mace': 6,\n",
       "             'painful': 20,\n",
       "             'dr_hibbert': 46,\n",
       "             'enjoy': 319,\n",
       "             'uh': 2381,\n",
       "             'slip': 66,\n",
       "             'novelty': 26,\n",
       "             'cube': 42,\n",
       "             'fake': 115,\n",
       "             'fly': 257,\n",
       "             'highly': 51,\n",
       "             'toxic': 18,\n",
       "             'chemical': 23,\n",
       "             'ironically': 11,\n",
       "             'sanitary': 4,\n",
       "             'face': 520,\n",
       "             'priceless': 22,\n",
       "             'cute': 128,\n",
       "             'everybody': 434,\n",
       "             'funniest': 10,\n",
       "             'king': 213,\n",
       "             'wantin': 5,\n",
       "             \"'\": 2780,\n",
       "             'nerve': 19,\n",
       "             'wife': 525,\n",
       "             'meet': 479,\n",
       "             'hour': 422,\n",
       "             'ago': 129,\n",
       "             'stink': 116,\n",
       "             'lousy': 131,\n",
       "             'operation': 59,\n",
       "             'quit': 223,\n",
       "             'gee': 127,\n",
       "             'handful': 11,\n",
       "             'peanut': 71,\n",
       "             'maude': 80,\n",
       "             'invitin': 2,\n",
       "             'wonderful': 221,\n",
       "             'night': 813,\n",
       "             'suggest': 51,\n",
       "             'kid': 1897,\n",
       "             'young': 313,\n",
       "             'fight': 371,\n",
       "             'music': 262,\n",
       "             'send': 365,\n",
       "             'chill': 28,\n",
       "             'spine': 22,\n",
       "             'act': 244,\n",
       "             'wet': 80,\n",
       "             'clothe': 117,\n",
       "             'dry': 87,\n",
       "             'martini': 10,\n",
       "             'good_lord': 61,\n",
       "             'glass': 160,\n",
       "             'pronounce': 29,\n",
       "             'whimsical': 3,\n",
       "             'jape': 3,\n",
       "             'season': 91,\n",
       "             'patient': 30,\n",
       "             'tolerant': 5,\n",
       "             'woman': 590,\n",
       "             'line': 288,\n",
       "             'cross': 93,\n",
       "             'stop': 1075,\n",
       "             'love': 1887,\n",
       "             'forget': 513,\n",
       "             'church': 206,\n",
       "             'stay': 519,\n",
       "             'scar': 18,\n",
       "             'inside': 219,\n",
       "             'notice': 115,\n",
       "             'strange': 61,\n",
       "             'admit': 113,\n",
       "             'hope': 504,\n",
       "             'respect': 126,\n",
       "             'sneak': 62,\n",
       "             'preview': 10,\n",
       "             'week': 468,\n",
       "             'sermon': 22,\n",
       "             'announcement': 43,\n",
       "             'pamphlet': 19,\n",
       "             'available': 35,\n",
       "             'newsrack': 1,\n",
       "             'include': 80,\n",
       "             'bible': 94,\n",
       "             'baffler': 1,\n",
       "             'satan': 46,\n",
       "             'boner': 11,\n",
       "             'grief': 13,\n",
       "             'teen': 44,\n",
       "             'cool': 467,\n",
       "             'fry': 85,\n",
       "             'hell': 420,\n",
       "             'lord': 211,\n",
       "             'compete': 23,\n",
       "             'squeaking': 1,\n",
       "             'homer_simpson': 423,\n",
       "             'shoe': 166,\n",
       "             'seat': 164,\n",
       "             'mrs': 147,\n",
       "             'lovejoy': 67,\n",
       "             'annual': 40,\n",
       "             'marriage': 225,\n",
       "             'retreat': 12,\n",
       "             'weekend': 97,\n",
       "             'catfish': 10,\n",
       "             'lake': 44,\n",
       "             'psychological': 9,\n",
       "             'counseling': 11,\n",
       "             'hang': 226,\n",
       "             'thread': 12,\n",
       "             'tune': 60,\n",
       "             'wish': 411,\n",
       "             'participate': 16,\n",
       "             'sign': 308,\n",
       "             'service': 130,\n",
       "             'attend': 29,\n",
       "             'tempting': 7,\n",
       "             'idea': 426,\n",
       "             'encounter': 10,\n",
       "             'fishing': 16,\n",
       "             'hello': 623,\n",
       "             'mrs_simpson': 117,\n",
       "             'suppose': 346,\n",
       "             'sitter': 23,\n",
       "             'dear': 308,\n",
       "             'find': 1264,\n",
       "             'babysitter': 28,\n",
       "             'kick': 184,\n",
       "             'tooth': 163,\n",
       "             'half': 287,\n",
       "             'tone': 27,\n",
       "             'young_lady': 39,\n",
       "             'taste': 167,\n",
       "             'hand': 580,\n",
       "             'wonder': 256,\n",
       "             'babysit': 7,\n",
       "             'ask': 566,\n",
       "             'desperate': 33,\n",
       "             'resort': 21,\n",
       "             'grampa': 271,\n",
       "             'feeb': 2,\n",
       "             'count': 140,\n",
       "             'dagnabit': 4,\n",
       "             'agin': 2,\n",
       "             'puttin': 17,\n",
       "             'trunk': 29,\n",
       "             'fever': 38,\n",
       "             'number': 369,\n",
       "             'stick': 309,\n",
       "             'finger': 125,\n",
       "             'electrical': 14,\n",
       "             'socket': 4,\n",
       "             'pine': 24,\n",
       "             'cleanser': 5,\n",
       "             'behave': 26,\n",
       "             'fall': 286,\n",
       "             'bathtub': 16,\n",
       "             'hurry': 113,\n",
       "             'list': 111,\n",
       "             'uh_huh': 124,\n",
       "             'smoke': 132,\n",
       "             'cigar': 24,\n",
       "             'gas': 155,\n",
       "             'fill': 214,\n",
       "             'er': 84,\n",
       "             'stretch': 31,\n",
       "             'leg': 166,\n",
       "             'general': 51,\n",
       "             'sherman': 20,\n",
       "             'ya': 873,\n",
       "             'wait_wait': 98,\n",
       "             'minute': 445,\n",
       "             'wait_minute': 319,\n",
       "             'part': 40,\n",
       "             'weigh': 30,\n",
       "             'upwards': 4,\n",
       "             'pound': 113,\n",
       "             'picture': 276,\n",
       "             'exactly': 175,\n",
       "             'freakishly': 2,\n",
       "             'hmmmm': 11,\n",
       "             'gentleman': 156,\n",
       "             'catch': 296,\n",
       "             'supermarket': 16,\n",
       "             'video': 123,\n",
       "             'store': 315,\n",
       "             'grab': 89,\n",
       "             'krusty': 563,\n",
       "             'burger': 54,\n",
       "             'head': 643,\n",
       "             'arcade': 8,\n",
       "             'kindly': 20,\n",
       "             'old_man': 152,\n",
       "             'trust': 143,\n",
       "             'advantage': 35,\n",
       "             'lis': 202,\n",
       "             'crazy': 381,\n",
       "             'topsy': 3,\n",
       "             'turvy': 3,\n",
       "             'wrong': 486,\n",
       "             'gut': 63,\n",
       "             'bleed': 37,\n",
       "             'gramp': 3,\n",
       "             'welcome': 442,\n",
       "             'reverend': 96,\n",
       "             'helen': 53,\n",
       "             'hel': 25,\n",
       "             'lo': 38,\n",
       "             'spit': 65,\n",
       "             'shine': 64,\n",
       "             'chance': 232,\n",
       "             'afraid': 356,\n",
       "             'reconcile': 2,\n",
       "             'bait': 16,\n",
       "             'hook': 70,\n",
       "             'honesty': 14,\n",
       "             'will': 753,\n",
       "             'away': 678,\n",
       "             'bowling': 63,\n",
       "             'expression': 27,\n",
       "             'turnout': 4,\n",
       "             'room': 480,\n",
       "             'introduce': 57,\n",
       "             'little_bit': 57,\n",
       "             'john': 110,\n",
       "             'gloria': 13,\n",
       "             'johnny': 51,\n",
       "             'boy': 1859,\n",
       "             'able': 112,\n",
       "             'cut': 412,\n",
       "             'manwise': 1,\n",
       "             'odor': 14,\n",
       "             'gin': 12,\n",
       "             'sour': 23,\n",
       "             'defeat': 36,\n",
       "             'press': 81,\n",
       "             'cook': 84,\n",
       "             'keep': 192,\n",
       "             'filthy': 39,\n",
       "             'profanely': 1,\n",
       "             'queen': 108,\n",
       "             'harpy': 1,\n",
       "             'eye': 576,\n",
       "             'beautiful': 343,\n",
       "             'save': 565,\n",
       "             'bring': 684,\n",
       "             'happiness': 50,\n",
       "             'pass': 268,\n",
       "             'collection': 52,\n",
       "             'plate': 71,\n",
       "             'ned': 171,\n",
       "             'god_bless': 35,\n",
       "             'underline': 3,\n",
       "             'passage': 10,\n",
       "             'gun': 208,\n",
       "             'ohhh': 79,\n",
       "             'drunk': 153,\n",
       "             'dress': 166,\n",
       "             'fault': 144,\n",
       "             'interrupt': 34,\n",
       "             'turn': 825,\n",
       "             'self': 123,\n",
       "             'center': 126,\n",
       "             'birthdays': 2,\n",
       "             'anniversarie': 1,\n",
       "             'holiday': 61,\n",
       "             'religious': 29,\n",
       "             'secular': 3,\n",
       "             'chew': 73,\n",
       "             'mouth': 200,\n",
       "             'gamble': 33,\n",
       "             'seedy': 2,\n",
       "             'bar': 334,\n",
       "             'bum': 61,\n",
       "             'lowlife': 2,\n",
       "             'blow': 251,\n",
       "             'nose': 142,\n",
       "             'towel': 48,\n",
       "             'put': 109,\n",
       "             'gallon': 17,\n",
       "             'chocolate': 112,\n",
       "             'brownie': 21,\n",
       "             'fudge': 32,\n",
       "             'chip': 84,\n",
       "             'write': 464,\n",
       "             'shopping': 35,\n",
       "             'aisle': 21,\n",
       "             'step': 188,\n",
       "             'carton': 13,\n",
       "             'change': 487,\n",
       "             'make': 715,\n",
       "             'noise': 64,\n",
       "             'wake': 180,\n",
       "             'honking': 2,\n",
       "             'scratch': 46,\n",
       "             'key': 168,\n",
       "             'wait': 767,\n",
       "             'toenail': 6,\n",
       "             'yellow': 73,\n",
       "             'tired': 142,\n",
       "             'chest': 37,\n",
       "             'luau': 3,\n",
       "             'mcbain': 32,\n",
       "             'cannon': 21,\n",
       "             'regulation': 9,\n",
       "             'department': 61,\n",
       "             'book': 528,\n",
       "             \"gettin_'\": 143,\n",
       "             'pretty': 456,\n",
       "             'late': 292,\n",
       "             'use': 646,\n",
       "             'have': 330,\n",
       "             'ethical': 5,\n",
       "             'crisis': 36,\n",
       "             'thirty': 226,\n",
       "             'clean': 287,\n",
       "             'seven': 279,\n",
       "             'incriminate': 5,\n",
       "             'evidence': 41,\n",
       "             'perfect': 288,\n",
       "             'crime': 125,\n",
       "             'fish': 180,\n",
       "             'selfishness': 2,\n",
       "             'anytime': 18,\n",
       "             'honest': 99,\n",
       "             'walk': 349,\n",
       "             'get_to': 755,\n",
       "             'husband': 307,\n",
       "             'ahead': 148,\n",
       "             'waste': 163,\n",
       "             'strength': 38,\n",
       "             'skillet': 1,\n",
       "             'butter': 70,\n",
       "             'ma': 96,\n",
       "             'coffee': 129,\n",
       "             'blowout': 2,\n",
       "             'casa': 3,\n",
       "             'frail': 2,\n",
       "             'joint': 23,\n",
       "             'twoish': 1,\n",
       "             'square': 94,\n",
       "             'funky': 15,\n",
       "             'noon': 20,\n",
       "             'exercise': 51,\n",
       "             'backwards': 33,\n",
       "             'spouse': 6,\n",
       "             'recommend': 28,\n",
       "             'counselor': 21,\n",
       "             'instance': 7,\n",
       "             'partner': 75,\n",
       "             'percent': 99,\n",
       "             'willing': 37,\n",
       "             'certificate': 16,\n",
       "             'frame': 53,\n",
       "             'word': 540,\n",
       "             'yank': 11,\n",
       "             \"comin_'\": 168,\n",
       "             'famous': 104,\n",
       "             'fisherman': 1,\n",
       "             'bald': 54,\n",
       "             'cable': 60,\n",
       "             'mackerel': 1,\n",
       "             'pal': 112,\n",
       "             'cherry': 28,\n",
       "             'chick': 82,\n",
       "             'obvious': 28,\n",
       "             'degrade': 1,\n",
       "             'set': 310,\n",
       "             'movement': 14,\n",
       "             'decade': 18,\n",
       "             'great': 1386,\n",
       "             'shut': 294,\n",
       "             'door': 276,\n",
       "             'haw_haw': 74,\n",
       "             'haw': 25,\n",
       "             'hellion': 4,\n",
       "             'belt': 67,\n",
       "             'nice': 669,\n",
       "             'tie': 141,\n",
       "             'nelson': 185,\n",
       "             'fail': 121,\n",
       "             'useless': 29,\n",
       "             'strong': 115,\n",
       "             'unpleasant': 12,\n",
       "             'remorse': 7,\n",
       "             'vile': 3,\n",
       "             'burlesque': 12,\n",
       "             'irrepressible': 3,\n",
       "             'youth': 38,\n",
       "             'bucket': 51,\n",
       "             'brush': 42,\n",
       "             'hard': 418,\n",
       "             'fast': 253,\n",
       "             'champion': 42,\n",
       "             'loser': 140,\n",
       "             'cause': 123,\n",
       "             'worlllld': 1,\n",
       "             'trouble': 224,\n",
       "             'expect': 115,\n",
       "             'represent': 47,\n",
       "             'hero': 167,\n",
       "             'weirdo': 21,\n",
       "             'worm': 29,\n",
       "             'selfish': 29,\n",
       "             'wow': 640,\n",
       "             'give': 654,\n",
       "             'fame': 24,\n",
       "             'breakfast': 113,\n",
       "             'toss': 48,\n",
       "             'secret': 272,\n",
       "             'fawcet': 1,\n",
       "             'boo': 72,\n",
       "             'hoo': 67,\n",
       "             'sad': 158,\n",
       "             'person': 198,\n",
       "             'fool': 160,\n",
       "             'sucker': 62,\n",
       "             'hee_hee': 37,\n",
       "             'hee': 21,\n",
       "             'yup': 20,\n",
       "             'dwell': 5,\n",
       "             'fury': 5,\n",
       "             'fella': 63,\n",
       "             'close': 367,\n",
       "             'foot': 247,\n",
       "             'tall': 54,\n",
       "             'arm': 151,\n",
       "             'tree': 220,\n",
       "             'steel': 27,\n",
       "             'cold': 146,\n",
       "             'shock': 44,\n",
       "             'hair': 331,\n",
       "             'red': 202,\n",
       "             'fire': 434,\n",
       "             'convention': 28,\n",
       "             'soon': 257,\n",
       "             'comic_strip': 44,\n",
       "             'fallout': 22,\n",
       "             'ward': 7,\n",
       "             'buy': 680,\n",
       "             'casper': 5,\n",
       "             'wimpy': 1,\n",
       "             'ghost': 67,\n",
       "             'equate': 1,\n",
       "             'friendliness': 1,\n",
       "             'wimpiness': 2,\n",
       "             'achieve': 25,\n",
       "             'popularity': 13,\n",
       "             'richie': 9,\n",
       "             'rich': 198,\n",
       "             'alike': 15,\n",
       "             'die': 554,\n",
       "             'hollow': 19,\n",
       "             'pursuit': 10,\n",
       "             'lighten': 21,\n",
       "             'radioactive_man': 53,\n",
       "             'rule': 172,\n",
       "             'knock': 148,\n",
       "             'sun': 96,\n",
       "             'hot': 314,\n",
       "             'dressed': 4,\n",
       "             'popular': 101,\n",
       "             'cartoon': 136,\n",
       "             'character': 125,\n",
       "             'discount': 24,\n",
       "             'ahem': 5,\n",
       "             'springfield': 979,\n",
       "             'mayor': 138,\n",
       "             'funny': 314,\n",
       "             'pump': 49,\n",
       "             'dollar': 424,\n",
       "             'local': 132,\n",
       "             'economy': 26,\n",
       "             'youthful': 11,\n",
       "             'high': 258,\n",
       "             'spirit': 114,\n",
       "             'impart': 5,\n",
       "             'glow': 25,\n",
       "             'war': 200,\n",
       "             'horse': 153,\n",
       "             'radiation': 22,\n",
       "             'jerk': 167,\n",
       "             'stand': 348,\n",
       "             'correct': 50,\n",
       "             'clear': 148,\n",
       "             'shriner': 2,\n",
       "             'punk': 45,\n",
       "             'diamond': 48,\n",
       "             'joe': 131,\n",
       "             'quimby': 77,\n",
       "             'excuse': 306,\n",
       "             'left': 106,\n",
       "             'vulcan': 3,\n",
       "             'ear': 113,\n",
       "             'utility': 15,\n",
       "             'tricorder': 1,\n",
       "             'light': 270,\n",
       "             'saber': 9,\n",
       "             'dude': 198,\n",
       "             'otto': 79,\n",
       "             'oooh': 137,\n",
       "             'comic_book': 77,\n",
       "             'school': 952,\n",
       "             'bus': 196,\n",
       "             'vampire': 47,\n",
       "             'post': 65,\n",
       "             'apocalyptic': 2,\n",
       "             'warzone': 1,\n",
       "             'buddy': 143,\n",
       "             'hodge': 2,\n",
       "             'play': 853,\n",
       "             'tv': 416,\n",
       "             'kill': 749,\n",
       "             'vietnam': 18,\n",
       "             'aaahh': 1,\n",
       "             'laramie': 15,\n",
       "             'cigarette': 69,\n",
       "             'steady': 23,\n",
       "             'combat': 13,\n",
       "             'evil': 139,\n",
       "             'whillikers': 1,\n",
       "             'wisht': 1,\n",
       "             'sixteen': 33,\n",
       "             'earth': 181,\n",
       "             \"y'know\": 87,\n",
       "             'actor': 50,\n",
       "             'dirk': 4,\n",
       "             'richter': 4,\n",
       "             'portrayal': 1,\n",
       "             'sordid': 6,\n",
       "             'detail': 26,\n",
       "             'question': 309,\n",
       "             'tasteful': 5,\n",
       "             'inject': 11,\n",
       "             'shrink': 12,\n",
       "             'serum': 3,\n",
       "             'issue': 74,\n",
       "             'finish': 182,\n",
       "             'tum': 6,\n",
       "             'tugger': 1,\n",
       "             'second': 406,\n",
       "             'national': 69,\n",
       "             'touring': 1,\n",
       "             'company': 168,\n",
       "             'cat': 238,\n",
       "             'ooh_ooh': 34,\n",
       "             'mask': 34,\n",
       "             'haunt': 25,\n",
       "             'bordello': 3,\n",
       "             'bullet': 57,\n",
       "             'riddled': 1,\n",
       "             'body': 204,\n",
       "             'vulture': 8,\n",
       "             'seventy': 89,\n",
       "             'imaginary': 21,\n",
       "             'tale': 57,\n",
       "             'marry': 310,\n",
       "             'larva': 2,\n",
       "             'grubby': 2,\n",
       "             'them': 545,\n",
       "             'bet': 247,\n",
       "             'million': 156,\n",
       "             'buck': 211,\n",
       "             'lad': 35,\n",
       "             'remind': 122,\n",
       "             'moment': 207,\n",
       "             'sell': 391,\n",
       "             'dozen': 27,\n",
       "             'lois': 7,\n",
       "             'lane': 32,\n",
       "             'superman': 27,\n",
       "             'see': 914,\n",
       "             'lariat': 1,\n",
       "             'dinner': 302,\n",
       "             'treat': 137,\n",
       "             'sport': 129,\n",
       "             'fine': 535,\n",
       "             'restaurant': 86,\n",
       "             'draw': 100,\n",
       "             'micha': 1,\n",
       "             'langelo': 1,\n",
       "             'usually': 93,\n",
       "             'bug': 103,\n",
       "             'mad': 221,\n",
       "             'pay_attention': 48,\n",
       "             'win': 627,\n",
       "             'apple': 133,\n",
       "             'woo_hoo': 203,\n",
       "             'gloat': 3,\n",
       "             'age': 170,\n",
       "             'sized': 13,\n",
       "             'electric': 64,\n",
       "             'lightbulb': 4,\n",
       "             'oven': 31,\n",
       "             'patty_selma': 31,\n",
       "             'slave': 37,\n",
       "             'free': 523,\n",
       "             'smoking': 25,\n",
       "             'month': 246,\n",
       "             'venus': 13,\n",
       "             'venu': 1,\n",
       "             'shield': 12,\n",
       "             'wash': 113,\n",
       "             'drip': 8,\n",
       "             'finally': 355,\n",
       "             'extra': 169,\n",
       "             'answer': 293,\n",
       "             'aw': 465,\n",
       "             'piece': 174,\n",
       "             'childhood': 31,\n",
       "             'forever': 223,\n",
       "             'ahh': 72,\n",
       "             'ooh': 520,\n",
       "             'deposit': 21,\n",
       "             'defray': 1,\n",
       "             'cost': 144,\n",
       "             'jumbo': 19,\n",
       "             'squishie': 1,\n",
       "             'dime': 26,\n",
       "             'learn': 482,\n",
       "             'trade': 66,\n",
       "             'americanize': 1,\n",
       "             'coin': 37,\n",
       "             'cent': 122,\n",
       "             'humiliating': 12,\n",
       "             'terrible': 174,\n",
       "             'car': 692,\n",
       "             'slow': 106,\n",
       "             'laugh': 244,\n",
       "             'buying': 5,\n",
       "             'sympathy': 5,\n",
       "             'ha': 164,\n",
       "             'pathetic': 43,\n",
       "             'lemonade': 27,\n",
       "             'suck': 206,\n",
       "             'product': 66,\n",
       "             'form': 128,\n",
       "             'crowd': 83,\n",
       "             'cheap': 84,\n",
       "             'beer': 501,\n",
       "             'sympathetic': 3,\n",
       "             'credit': 78,\n",
       "             'liquor': 42,\n",
       "             'license': 73,\n",
       "             'ugh': 56,\n",
       "             'dog': 649,\n",
       "             'ticket': 217,\n",
       "             'thirsty': 13,\n",
       "             'ell': 3,\n",
       "             'offense': 28,\n",
       "             'officer': 66,\n",
       "             'poor': 227,\n",
       "             'earn': 81,\n",
       "             'nazi': 12,\n",
       "             'smasher': 2,\n",
       "             'chore': 26,\n",
       "             'mix': 67,\n",
       "             'whitewash': 4,\n",
       "             'eh': 491,\n",
       "             'burt': 12,\n",
       "             'apricot': 2,\n",
       "             'almond': 12,\n",
       "             'paste': 14,\n",
       "             'sauerkraut': 4,\n",
       "             'candy': 192,\n",
       "             'brother': 319,\n",
       "             'asa': 7,\n",
       "             'grenade': 6,\n",
       "             'kaiser': 4,\n",
       "             'bill': 215,\n",
       "             'delivery': 41,\n",
       "             'uncle': 90,\n",
       "             'sam': 20,\n",
       "             'harrison': 6,\n",
       "             'brooklyn': 9,\n",
       "             'bob': 167,\n",
       "             'reggie': 3,\n",
       "             'be': 357,\n",
       "             'stuck': 49,\n",
       "             'ribbon': 24,\n",
       "             \"ma'am\": 88,\n",
       "             'start': 790,\n",
       "             'yard': 84,\n",
       "             'barley': 3,\n",
       "             'pop': 135,\n",
       "             'weed': 11,\n",
       "             'one': 133,\n",
       "             'careful': 72,\n",
       "             'watch': 701,\n",
       "             'story': 431,\n",
       "             'genuinely': 1,\n",
       "             'arouse': 4,\n",
       "             'merciful': 6,\n",
       "             'heaven': 131,\n",
       "             'iodine': 3,\n",
       "             'listen': 599,\n",
       "             'lady': 449,\n",
       "             'yaaauuuuggghhh': 1,\n",
       "             'glick': 7,\n",
       "             'sludge': 5,\n",
       "             'certainly': 108,\n",
       "             'collect': 50,\n",
       "             'downspout': 1,\n",
       "             'bat': 62,\n",
       "             'beulah': 2,\n",
       "             'wedding': 184,\n",
       "             ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68062dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b68208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea66c6c",
   "metadata": {},
   "source": [
    "### Parameter Explanation:\n",
    "min_count=20: This parameter specifies the minimum number of occurrences a word must have to be included in the model's vocabulary. Words that appear fewer than 20 times will be ignored. This helps in reducing noise and focusing on more significant words.\n",
    "\n",
    "window=2: This parameter sets the maximum distance between the current and predicted word within a sentence. A window size of 2 means that the model will consider the two words before and two words after the target word during training.\n",
    "\n",
    "size=300: This parameter defines the dimensionality of the word vectors. A size of 300 means each word will be represented by a vector with 300 dimensions, allowing for rich semantic representation.\n",
    "\n",
    "sample=6e-5: This parameter is used for down-sampling frequent words. Words that occur very frequently (like \"the\", \"and\", etc.) will be randomly down-sampled based on this threshold to improve training efficiency and reduce overfitting.\n",
    "\n",
    "alpha=0.03: This parameter sets the initial learning rate for training the model. A value of 0.03 is common for Word2Vec models.\n",
    "\n",
    "min_alpha=0.0007: This parameter sets the minimum learning rate. The learning rate will decrease linearly from the initial alpha to min_alpha during training. This helps the model to converge effectively.\n",
    "\n",
    "negative=20: This parameter specifies the number of negative samples to be drawn for each positive sample during training. A value of 20 indicates that for every positive word association, 20 negative samples will be considered. This helps in improving the model's performance by contrasting positive and negative examples.\n",
    "\n",
    "workers=cores-1: This parameter specifies the number of worker threads to train the model. Using cores-1 utilizes all but one of the available CPU cores, allowing for parallel processing and speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0f000fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:50:15: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-10-19T09:50:15.139142', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    vector_size=300,  # Changed 'size' to 'vector_size'\n",
    "    sample=6e-5, \n",
    "    alpha=0.03, \n",
    "    min_alpha=0.0007, \n",
    "    negative=20,\n",
    "    workers=cores-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f08da975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:50:17: collecting all words and their counts\n",
      "INFO - 09:50:17: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 09:50:17: PROGRESS: at sentence #10000, processed 61697 words, keeping 9516 word types\n",
      "INFO - 09:50:17: PROGRESS: at sentence #20000, processed 127312 words, keeping 14382 word types\n",
      "INFO - 09:50:17: PROGRESS: at sentence #30000, processed 187772 words, keeping 17441 word types\n",
      "INFO - 09:50:17: PROGRESS: at sentence #40000, processed 243265 words, keeping 20120 word types\n",
      "INFO - 09:50:17: PROGRESS: at sentence #50000, processed 303120 words, keeping 22550 word types\n",
      "INFO - 09:50:18: PROGRESS: at sentence #60000, processed 363858 words, keeping 24819 word types\n",
      "INFO - 09:50:18: PROGRESS: at sentence #70000, processed 425311 words, keeping 26987 word types\n",
      "INFO - 09:50:18: PROGRESS: at sentence #80000, processed 485433 words, keeping 28822 word types\n",
      "INFO - 09:50:18: collected 29694 word types from a corpus of 523538 raw words and 85955 sentences\n",
      "INFO - 09:50:18: Creating a fresh vocabulary\n",
      "INFO - 09:50:18: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 3324 unique words (11.19% of original 29694, drops 26370)', 'datetime': '2024-10-19T09:50:18.085517', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 09:50:18: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 438021 word corpus (83.67% of original 523538, drops 85517)', 'datetime': '2024-10-19T09:50:18.085978', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 09:50:18: deleting the raw counts dictionary of 29694 items\n",
      "INFO - 09:50:18: sample=6e-05 downsamples 1197 most-common words\n",
      "INFO - 09:50:18: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 199343.36670965044 word corpus (45.5%% of prior 438021)', 'datetime': '2024-10-19T09:50:18.092826', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 09:50:18: estimated required memory for 3324 words and 300 dimensions: 9639600 bytes\n",
      "INFO - 09:50:18: resetting layer weights\n",
      "INFO - 09:50:18: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-19T09:50:18.108789', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fdf2728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:50:19: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3324 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2024-10-19T09:50:19.480978', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 09:50:20: EPOCH 0: training on 523538 raw words (199114 effective words) took 0.5s, 388143 effective words/s\n",
      "INFO - 09:50:20: EPOCH 1: training on 523538 raw words (199798 effective words) took 0.3s, 632893 effective words/s\n",
      "INFO - 09:50:20: EPOCH 2: training on 523538 raw words (199347 effective words) took 0.4s, 521019 effective words/s\n",
      "INFO - 09:50:21: EPOCH 3: training on 523538 raw words (199277 effective words) took 0.4s, 455206 effective words/s\n",
      "INFO - 09:50:21: EPOCH 4: training on 523538 raw words (199404 effective words) took 0.3s, 614231 effective words/s\n",
      "INFO - 09:50:21: EPOCH 5: training on 523538 raw words (199024 effective words) took 0.4s, 455808 effective words/s\n",
      "INFO - 09:50:22: EPOCH 6: training on 523538 raw words (199234 effective words) took 0.4s, 452965 effective words/s\n",
      "INFO - 09:50:22: EPOCH 7: training on 523538 raw words (198706 effective words) took 0.3s, 669885 effective words/s\n",
      "INFO - 09:50:23: EPOCH 8: training on 523538 raw words (199095 effective words) took 0.4s, 494890 effective words/s\n",
      "INFO - 09:50:23: EPOCH 9: training on 523538 raw words (199284 effective words) took 0.4s, 459734 effective words/s\n",
      "INFO - 09:50:24: EPOCH 10: training on 523538 raw words (199588 effective words) took 0.4s, 483072 effective words/s\n",
      "INFO - 09:50:24: EPOCH 11: training on 523538 raw words (199140 effective words) took 0.3s, 589338 effective words/s\n",
      "INFO - 09:50:24: EPOCH 12: training on 523538 raw words (199213 effective words) took 0.4s, 446444 effective words/s\n",
      "INFO - 09:50:25: EPOCH 13: training on 523538 raw words (199458 effective words) took 0.5s, 432565 effective words/s\n",
      "INFO - 09:50:25: EPOCH 14: training on 523538 raw words (199304 effective words) took 0.3s, 668931 effective words/s\n",
      "INFO - 09:50:26: EPOCH 15: training on 523538 raw words (199544 effective words) took 0.4s, 562826 effective words/s\n",
      "INFO - 09:50:26: EPOCH 16: training on 523538 raw words (199376 effective words) took 0.4s, 485147 effective words/s\n",
      "INFO - 09:50:27: EPOCH 17: training on 523538 raw words (199471 effective words) took 0.4s, 457683 effective words/s\n",
      "INFO - 09:50:27: EPOCH 18: training on 523538 raw words (199314 effective words) took 0.3s, 591557 effective words/s\n",
      "INFO - 09:50:27: EPOCH 19: training on 523538 raw words (199328 effective words) took 0.4s, 454422 effective words/s\n",
      "INFO - 09:50:28: EPOCH 20: training on 523538 raw words (199503 effective words) took 0.4s, 474290 effective words/s\n",
      "INFO - 09:50:28: EPOCH 21: training on 523538 raw words (199355 effective words) took 0.5s, 437299 effective words/s\n",
      "INFO - 09:50:29: EPOCH 22: training on 523538 raw words (199549 effective words) took 0.3s, 607732 effective words/s\n",
      "INFO - 09:50:29: EPOCH 23: training on 523538 raw words (199574 effective words) took 0.4s, 464847 effective words/s\n",
      "INFO - 09:50:30: EPOCH 24: training on 523538 raw words (199585 effective words) took 0.5s, 418393 effective words/s\n",
      "INFO - 09:50:30: EPOCH 25: training on 523538 raw words (199459 effective words) took 0.5s, 428523 effective words/s\n",
      "INFO - 09:50:30: EPOCH 26: training on 523538 raw words (199777 effective words) took 0.3s, 583479 effective words/s\n",
      "INFO - 09:50:31: EPOCH 27: training on 523538 raw words (199247 effective words) took 0.4s, 447250 effective words/s\n",
      "INFO - 09:50:31: EPOCH 28: training on 523538 raw words (199380 effective words) took 0.5s, 439944 effective words/s\n",
      "INFO - 09:50:32: EPOCH 29: training on 523538 raw words (199238 effective words) took 0.3s, 642869 effective words/s\n",
      "INFO - 09:50:32: Word2Vec lifecycle event {'msg': 'training on 15706140 raw words (5980686 effective words) took 12.7s, 472226 effective words/s', 'datetime': '2024-10-19T09:50:32.148041', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]', 'platform': 'macOS-15.0.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.21 mins\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d46c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/q0txn0fd5qj57z36hsjf2yxh0000gn/T/ipykernel_17561/514372312.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 18:19:18: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d24a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rude', 0.6942655444145203),\n",
       " ('marge', 0.6873096227645874),\n",
       " ('sweetheart', 0.6872086524963379),\n",
       " ('creepy', 0.680105984210968),\n",
       " ('gee', 0.671907365322113),\n",
       " ('jealous', 0.6673969626426697),\n",
       " ('snuggle', 0.6642676591873169),\n",
       " ('depressed', 0.6641891002655029),\n",
       " ('terrific', 0.6641192436218262),\n",
       " ('hammock', 0.6567658185958862)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92255c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('select', 0.7401379346847534),\n",
       " ('congratulation', 0.7341328859329224),\n",
       " ('governor', 0.710042417049408),\n",
       " ('easily', 0.7082727551460266),\n",
       " ('montgomery_burn', 0.7031835317611694),\n",
       " ('recent', 0.6977173686027527),\n",
       " ('defeat', 0.696308970451355),\n",
       " ('council', 0.6908103227615356),\n",
       " ('united_states', 0.6816267967224121),\n",
       " ('threat', 0.6799395084381104)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer_simpson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b72f13b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humiliate', 0.7087414860725403),\n",
       " ('snuggle', 0.7066437005996704),\n",
       " ('becky', 0.698948085308075),\n",
       " ('rude', 0.6972123384475708),\n",
       " ('homer', 0.6873096227645874),\n",
       " ('affair', 0.6860363483428955),\n",
       " ('brunch', 0.6815887689590454),\n",
       " ('grownup', 0.6801910400390625),\n",
       " ('badly', 0.6784240007400513),\n",
       " ('depressed', 0.6776086091995239)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5325fc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sauce', -0.009900564327836037),\n",
       " ('silver', -0.010088246315717697),\n",
       " ('east', -0.021327510476112366),\n",
       " ('liberty', -0.025091029703617096),\n",
       " ('duff', -0.03209099918603897),\n",
       " ('kent_brockman', -0.034821219742298126),\n",
       " ('thee', -0.039052318781614304),\n",
       " ('north', -0.04283853620290756),\n",
       " ('john', -0.051570214331150055),\n",
       " ('low', -0.05474331974983215)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(negative=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e684c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68530107"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e07190ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61052835"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bart', 'nelson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f11830ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 11:19:06: vectors for words {'kearney'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jimbo'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c542f2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nelson'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b51d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d57c4866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('see', 0.5720261335372925),\n",
       " ('admire', 0.5511806011199951),\n",
       " ('man', 0.5392982363700867)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ae18205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7240086793899536),\n",
       " ('mom', 0.6408773064613342),\n",
       " ('surprised', 0.6376112699508667)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3c52486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a58321c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def tsnescatterplot(model, word, list_names):\n",
    "    # Create an array to hold the word vectors\n",
    "    arrays = np.empty((0, model.vector_size))\n",
    "    \n",
    "    # Get the vector for the specified word\n",
    "    if word in model.wv.key_to_index:\n",
    "        wrd_vector = model.wv[word].reshape(1, -1)\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # Get vectors for the list of names\n",
    "    for wrd in list_names:\n",
    "        if wrd in model.wv.key_to_index:\n",
    "            wrd_vector = model.wv[wrd].reshape(1, -1)\n",
    "            arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # Ensure arrays has sufficient dimensions for PCA\n",
    "    if arrays.shape[0] > 1:  # Check if we have more than one vector\n",
    "        # Reduces the dimensionality using PCA\n",
    "        reduc = PCA(n_components=min(2, arrays.shape[0]-1)).fit_transform(arrays)  # Use 2 or less components\n",
    "    else:\n",
    "        raise ValueError(\"Not enough data points to perform PCA.\")\n",
    "    \n",
    "    # t-SNE implementation can follow here...\n",
    "    # e.g., using the reduc array for t-SNE visualizations\n",
    "\n",
    "# Call the function with the Word2Vec model and your words\n",
    "tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72a3d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfef890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['psy', 'cho', 'ma', 'tic']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f43ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
